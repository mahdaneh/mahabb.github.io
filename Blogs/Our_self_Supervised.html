<html>
<head>
  <link rel="stylesheet" href="../css/blog_style.css">
   
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
      
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>
<body>
  <div class="wrapper">

    <header class="TOC">
      <h2> Table of Content </h2>
      <ul >
      
      <li><h3> <a href="#s1"> 1. Introduction </a></h3></li>
        <li><h3> <a href="#s2"> 2. Our framework in nutshell </a></h3></li>
        <ul><li><h3> <a href="#s1"> 2.1. Tricks </a></h3></li></ul>
      
    </ul>

    <a href="../index.html"> 
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-house-fill" viewBox="0 0 16 16">
      <path fill-rule="evenodd" d="M8 3.293l6 6V13.5a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 2 13.5V9.293l6-6zm5-.793V6l-2-2V2.5a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 .5.5z"/>
      <path fill-rule="evenodd" d="M7.293 1.5a1 1 0 0 1 1.414 0l6.647 6.646a.5.5 0 0 1-.708.708L8 2.207 1.354 8.854a.5.5 0 1 1-.708-.708L7.293 1.5z"/>
    </svg>
    Home</a>
    
    
    </header>
<section>

<h1>Self-supervised framework for generating pseudo-labels</h1>

<h2 id ="s1">1. Introduction</h2>

<p> <a href='./Blogs/Object_Detector.html'>In previous blog post</a>, 
  we explained how missing-label instances or UPIs (Unlabelled Positive Instances) can create 
  false negative signals, causing performance declination of a modern object detector, e.g. YOLO.
   Here, we propose to mitigate this challenge through generating pseudo-labels for the UPIs to reduce the number of false negative signals. 
</p>
<p>Briefly, our method is an end-to-end solution, meaning that during training of YOLO, it recognizes UPIs, then generates pseudo-labels. 
To this end, we incorporate a pre-trained proxy model (Augmented CNN-- a vanilla CNN with an extra class added to its output) to indicate whether the ROIs estimated by YOLO (during its training) contain an object of interest or not. 
If the proxy model classifies an ROI as its extra class (meaning it contains a not-interesting object), it is discarded, otherwise, the coordinate information of ROI along with its estimated class and its associated confidence provided by A-CNN are considered as a pseudo-label.
</p>
<p>
 In Fig 1, we show the overall pip-line of our proposed method for generating pseudo-labels during training YOLO,
 where it simultaneously is trained on both ground truths and the generated pseudo-labels.
</p>


<figure>
<img width="50%" src="../images/img_object_detector/Ours.png" class="center"> 
<figcaption>Figure 1: Our proposed method incorporates a pre-trained Augmented CNN to generate pseudo-labels on-the-fly in an end-to-end fashion.</figcaption>
</figure>

  <h2 id="s2"> 2. Our framework in nutshell</h2>

In this post, we briefly explain the stages of our proposed framework. Please refer to our <a href="https://arxiv.org/abs/2005.11549"> Arxiv paper</a>
  to read with more details.

<ul>
  <li><strong>Stage 1</strong>: in training phase (at a given epoch), YOLO outputs some estimated ROIs, which may (may not) contains an OOI. These ROIs from the given image are extracted. </li>
  <li><strong>Stage 2</strong>: feeding a pre-trained A-CNN with the extracted ROIs,
    they are classified as one of the pre-defined categories (corresponding to the categories of objects of interest) or reject them (equivalent to the A-CNN's extra class).
  The ROIs that are <b>classified confidently</b> as one of the pre-defined categories are forwarded to the next stage.</li>

  <li><strong>Stage 3</strong>: for the ROIs passed from the previous stage, we generate new pseudo-labels, each label includes the coordinate info of a ROI, objectness score, and category.
    Two latter values are the maximum predictive confidence and its corresponding class estimated by A-CNN for the given ROI. </li>
  <li><strong> Stage 4</strong>: the generated pseudo-labels and the ground truth are collectively used for training YOLO.</li>

</ul>

  <h2 id="s3"> 2.1. Tricks </h2>
  <p>There are some tricks related to A-CNN to get the above framework working.</p>

  <h3> Extracted ROIs with different sizes</h3>
  <p>
  Originally, the CNNs that have some FC layers can not process inputs with different sizes since the FC layers in the architecture can accept the inputs with identical size.
    However, feeding the convolution layers
  with different sizes incur outputs with different sizes, which are then passed to the successive FC layer in such an architecture.
    To address this issue, <a href="https://arxiv.org/abs/1406.4729">SPP (Spatial Pyramid Pooling)</a> layer is proposed, which allows a CNN process inputs data with various sizes.

  Using SPP, we then able to process the ROIs with different sizes. However, from technical view point (pytorch), we can not have a batch of inputs with different sizes.
  To circumvent this challenge, we used "k-means trick" so that minimized the zero-pad added to the images. In this trick, the input samples with different sizes are
    clustered so that the samples with similar sizes are placed
  in a cluster. Then, the samples of a cluster are padded so that they all have an identical size. Note that simply using naive padding, i.e. padding images to all have size of the largest image, is not recommended since it can create inputs with immense zero-margin, especially for tiny images.
  </p>

  <h3>Generate pseudo-labels only for the confident predictions</h3>
  <p>
  To create high confidence prediction,
    we used patch-drop to create different versions of an ROI, then averaging their predictions by the A-CNN, which then lead to better predictions
  in terms of calibration and reducing error. Later, we accept only those predictions with high confidence (larger than a threshold).
    Therefore, the chance of misclassification can be reduced, avoid generation of incorrect pseudo-labels.
  </p>
</section>
</div>
</body>
</html>
