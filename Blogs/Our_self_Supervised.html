<html>
<head>
  <link rel="stylesheet" href="../css/blog_style.css">
   
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
      
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>
<body>
  <div class="wrapper">

    <header class="TOC">
      <h2> Table of Content </h2>
      <ul >
      
      <li><h3> <a href="#s1"> 1. Introduction </a></h3></li>
        <li><h3> <a href="#s2"> 2. Our framework in nutshell </a></h3></li>
        <ul><li><h3> <a href="#s1"> 2.1. Tricks </a></h3></li></ul>
      
    </ul>
    <a href="../index.html"> 
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-house-fill" viewBox="0 0 16 16">
      <path fill-rule="evenodd" d="M8 3.293l6 6V13.5a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 2 13.5V9.293l6-6zm5-.793V6l-2-2V2.5a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 .5.5z"/>
      <path fill-rule="evenodd" d="M7.293 1.5a1 1 0 0 1 1.414 0l6.647 6.646a.5.5 0 0 1-.708.708L8 2.207 1.354 8.854a.5.5 0 1 1-.708-.708L7.293 1.5z"/>
    </svg>
    Home</a>
    
    
    </header>
<section>

<h1>Self-supervised framework for generating pseudo-labels</h1>

<h2 id ="s1">1. Introduction</h2>

<p> <a href='./Blogs/Object_Detector.html'>In previous blog post</a>, 
  we explained how missing-label instances or UPIs (Unlabelled Positive Instances) can create 
  false negative signals, causing performance declination of a modern object detector, e.g. YOLO.
   Here, we propose to mitigate this challenge through generating pseudo-labels for the UPIs to reduce the number of false negative signals. 
</p>
<p>Briefly, our method is an end-to-end solution, meaning that during training of YOLO, it recognizes UPIs, then generates pseudo-labels. 
To this end, we incorporate a pre-trained proxy model (Augmented CNN-- a vanilla CNN with an extra class added to its output) to indicate whether the ROIs estimated by YOLO (during its training) contain an object of interest or not. 
If the proxy model classifies an ROI as its extra class (meaning it contains a not-interesting object), it is discarded, otherwise, the coordinate information of ROI along with its estimated class and its associated confidence provided by A-CNN are considered as a pseudo-label.
</p>
<p>
 In Fig 1, we show the overall pip-line of our proposed method for generating pseudo-labels during training YOLO,
 where it simultaneously is trained on both ground truths and the generated pseudo-labels.
</p>


<figure>
<img width="50%" src="../images/img_object_detector/Ours.png" class="center"> 
<figcaption>Figure 1: Our proposed method incorporates a pre-trained Augmented CNN to generate pseudo-labels on-the-fly in an end-to-end fashion.</figcaption>
</figure>

  <h2 id="s2"> 2. Our framework in nutshell</h2>

In this post, we briefly explain the stages of our proposed framework. Please refer to our <a href="https://arxiv.org/abs/2005.11549"> Arxiv paper</a> to read about more details.

<ul>
  <li><strong>Stage 1</strong>: In training phase at a given epoch, YOLO outputs some estimated ROIs. They either contains an OOI or not. Then, we extract these ROIs from the given image. </li>
  <li><strong>Stage 2</strong>: The extracted ROIs are fed to an pre-trained A-CNN,
    so that it predicts whether the ROIs contain an OOI through either classifying them as one of it pre-defined categories or reject them (equivalent to classifying a ROI to its extra class).
  The ROIs classified as one of pre-defined categories (corresponding to the categories of objects of interest) are forwarded to the next stage.</li>

  <li><strong>Stage 3</strong>: If an ROI <b>classified confidently</b> as one of the categories of interest, then a new pseudo-label is created, included coordinate info of the ROI, objectness score, and category.
    Two latter values are the maximum predictive confidence and its corresponding class estimated by A-CNN for given ROI. </li>
  <li><strong> Stage 4</strong>: the generated pseudo-labels and ground truth are used for training YOLO.</li>

</ul>

  <h2 id="s3"> 2.1. Tricks </h2>
  <p>There are some tricks related to A-CNN to get the above framework working.</p>

  <h3> Extracted ROIs with different sizes</h3>
  <p>
  Originally, the CNNs that have some FC layers can not process inputs with different sizes since the FC layers in the architecture can accept only the inputs having identical size. However, feeding the convolution layers
  with different sizes will incur outputs with different sizes, which are indeed the input for the successive FC layer in such an architecture. To address this issue, <a href="https://arxiv.org/abs/1406.4729">SPP (Spatial Pyramid Pooling)</a> layer is proposed.
  Using SPP, we then able to process ROIs with different sizes. However, from technical view point (pytorch), we can not have a batch of inputs with different sizes.
  To circumvent this challenge, we used the k-means trick. In this trick, the input samples with different sizes are clustered so that the samples with similar sizes are placed
  in a cluster. Then, the samples of a cluster are padded so that they all have identical size. Note that simply using naive padding, i.e. padding images to all have size of the largest image, is not recommended since it can create inputs with immense zero-margin, especially for tiny images.
  </p>
  <h3>Generate pseudo-labels only for confident predictions</h3>
  <p>
  To create high confidence prediction, we used patch-drop to creat different versions of an ROI, then averaging predictions of A-CNN on all these version can lead to better
  (in terms of calibration) prediction. Later, we only accept the predictions that are large enough (larger than a threshold). Therefore, the chance of misclassification can be reduced to avoid generate incorrect pseudo-labels.
  </p>
</section>
</div>
</body>
</html>
