---
layout: default
---

<head>
<!--<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>-->
  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

    <section>
      <h1>Performance degradation of a SOTA object detecor trained on a dataset with missing-labels</h1>
  <h2> Introduction</h2>
<p>
  
  Modern CNN-based object detectors such as faster R-CNN and YOLO achieve remarkable performance when trained 
  on fully labeled large-scale datasets. 
  On the one hand, collecting a dataset with full annotations, specifically bounding boxes around Objects of Interest (OoI), can be a tedious and costly process.
  On the other hand, the object detectors such as R-CNN and YOLO show that their performance is dependent on having access to such fully labeled datasets. 
  In other words, training them on partially labelled datasets (i.e. containing instances with missing-labels) leads to a drop in generalization performance.


  
  

</p>
  
    Several scenarios can lead to a parially labelled dataset:
    <ul>
      <li><b>Unintentional error in labelling</b></li>
      <li> <b>Partial annotation policy </b>:To reduce annotation cost, "partial annotation policy" considers 
        to annotate only one instance of each object presented in a given image. For example, in an image containing a herd of sheep, only one sheep is annotated, instead of
        fully annotating all the sheep instances in the image. This policy causes some missing bounding box
        annotations but interestingly no missing image-level labels as at least one instance of each existing object in the given image is annotated.</li>
      <li><b>Merged dataset</b>  we aim at combining several datasets from similar (or the same) contexts but with disjoint 
        (or partially disjoint) sets of Objects of Interest (OoI), in order to <b>efficiently</b> construct a larger dataset that
        includes a wider range of objects of interest. For instance, Kitti and German Traffic Signs are
        two different datasets with two disjoint sets of OoIs that could be merged to cover a wider spectrum of objects appearing on roads. Furthermore,
        merging the datasets that include the object of interest from across different domains --i.e. the objects appeared in different poses, illuminations, styles, 
        etc-- have the potential to also mitigate the domain-shift challenge.</li>
      
      </ul>
      
      
      <p>Before explaining how missing-label instances, which we call them Unlabelled Positive Instances (<b>UPIs</b>),
 do negatively influence the performance of YOLO, let to briefly explain some elements of YOLO along with their notations.</p>
      
      
      <p>YOLO divides a given image $I$ 
      into $g\times g$ grids, then for each grid $G_{ij}$, it estimates $A$ different bounding-boxes,
      where each of them is a $5+K$-dimensional vector, encompassing the estimated coordinate information of the box 
      ($\mathbf{r}^a_{G_{ij}}=\left[\hat{x}^a_{G_{ij}},\hat{y}^a_{G_{ij}}, \hat{w}^a_{G_{ij}}, \hat{h}^a_{G_{ij}} \right]$),
      the objectiveness probability ($p(O|\mathbf{r}^a_{G_{ij}})$), and a $K$-dimensional vector as the probabilities over $K$ object categorizes ($\textbf{p}(c|\mathbf{r}^a_{G_{ij}})\in[0,1]^K$) with $a\in\{1,\dots A\}$. Therefore, the output of YOLO will be a tensor of size $\left[g,g,A,5+K\right]$.
      Moreover, for each grid ${G_{ij}}$, a set of pre-defined bounding-boxes (called anchors) with different aspect ratios and scales is considered. 
        YOLO learns to estimates the bounding-boxes w.r.t these pre-defined anchors.</p>
      
      
      <img width="70%" src="/Blogs/img_object_detector/YOLO.jpg">




      
      <h2> The Impact of Missing-label Instances on Performance</h2>
      
While UPIs do not contribute to the training of YOLO through their "class" and "coordinate" losses, they can adversely contribute through their "object" loss. In contrast to class and coordinate losses, the object loss is computed for all anchors of all grids, whether they contain any ground-truths or not. The computation of object loss for UPIs causes false-negative training signals, leading to a degradation of performance. More specifically, during the training of YOLO, the detector may be able to correctly localize a UPI, estimating its corresponding anchor with $p(O)\sim1$. However, since it has no associated ground-truth label, it is given true object label zero, i.e. $t=0$. This false-negative training signal forces the network to learn a UPI as a negative or not-interesting object even though the model can correctly localize and recognize it. Ultimately, such a false negative signal from a UPI can confuse the network since the LPIs (Labelled Positive Instance) of the same object category forces the network to learn it as an object of interest while the UPIs from the same object category encourage the network to learn it as a not-interesting (negative) object. Therefore, such false-negative signals can cause a drop in the performance of YOLO.      
 

<h2> How to Overcome This Problem</h2>
Generally speaking, there are two solutions for this problem arise from UPIs; ignore the false negative signals (<a href="https://arxiv.org/abs/1806.06986">Wu et. al. </a>) or correct them. In our <a href="https://arxiv.org/abs/2005.11549"> article </a>, We not only correct these false-negative signals, which are arisen from the object loss but also create more training signals from the coordinate and class losses through generating some pseudo-labels for the UPIs. 
     
    </section>

