---
layout: post
---

<head>
  <!--<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>-->
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
      
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  
  </head>
  
    <section>
      <h1>Performance degradation of a SOTA object detector trained on a partially-labeled dataset</h1>
  <h2> Introduction</h2>
<p>
  
  Modern CNN-based object detectors such as faster R-CNN and YOLO achieve remarkable performance when trained 
  on fully labeled large-scale datasets. 
  On the one hand, collecting a dataset with full annotations, specifically bounding boxes around Objects of Interest (OoI), can be a tedious and costly process.
  On the other hand, the object detectors such as R-CNN and YOLO show that their performance is dependent on having access to such fully labeled datasets. 
  In other words, training them on partially labelled datasets (i.e. containing instances with missing-labels) leads to a drop in generalization performance.


  
  <h2>When do datasets for object detection tasks contain missing-label instances?</h2>

</p>
  
    Several scenarios can lead to a partially labelled dataset:
    <ul>
      <li><b>Unintentional error in labelling</b></li>
      <li> <b>Partial annotation policy </b>:To reduce annotation cost, "partial annotation policy" considers 
        to annotate only one instance of each object presented in a given image. For example, in an image containing a herd of sheep, only one sheep is annotated, instead of
        fully annotating all the sheep instances in the image. This policy causes some missing bounding box
        annotations but interestingly no missing image-level labels as at least one instance of each existing object in the given image is annotated.</li>
        
      <li><b>Merged dataset</b>  Combining several datasets from similar (or the same) contexts but with disjoint 
        (or partially disjoint) sets of Objects of Interest (OoI) for <b>quickly</b> constructing a larger dataset that
        includes a wider range of objects of interest. For instance, <a href="http://www.cvlibs.net/datasets/kitti/">Kitti</a> and <a href="https://bdd-data.berkeley.edu/">BDDK100</a> are
        two different datasets for roads with two disjoint sets of OoIs that could be merged to cover a wider spectrum of variations (light, style, and etc ) and various road objects.
      <img width="50%" src="../images/img_object_detector/merged.png" class="center">Figure 1: Merging several datasets to quickly building a larger dataset with wider range of variations and object of interest.</img></li>
      </ul>
      
      <h2>Notations</h2>
      
      <p>Before explaining how missing-label instances, which we call them Unlabelled Positive Instances (<b>UPIs</b>),
 do negatively influence the performance of YOLO, let to briefly explain some elements of YOLO along with their notations.</p>
      
      
      <p>YOLO divides a given image $I$ 
      into $g\times g$ grids, then for each grid $G_{ij}$, it estimates $A$ different bounding-boxes,
      where each of them is a $5+K$-dimensional vector, encompassing the estimated coordinate information of the box 
      ($\mathbf{r}^a_{G_{ij}}=\left[\hat{x}^a_{G_{ij}},\hat{y}^a_{G_{ij}}, \hat{w}^a_{G_{ij}}, \hat{h}^a_{G_{ij}} \right]$),
      the objectiveness probability ($p(O|\mathbf{r}^a_{G_{ij}})$), and a $K$-dimensional vector as the probabilities over $K$ object categorizes ($\textbf{p}(c|\mathbf{r}^a_{G_{ij}})\in[0,1]^K$) with $a\in\{1,\dots A\}$. Therefore, the output of YOLO will be a tensor of size $\left[g,g,A,5+K\right]$.
      Moreover, for each grid ${G_{ij}}$, a set of pre-defined bounding-boxes (called anchors) with different aspect ratios and scales is considered. 
        YOLO learns to estimates the bounding-boxes w.r.t these pre-defined anchors.</p>
      <div class="parent">
        <div class="text">
      <figure>
      <img width="70%" src="../images/img_object_detector/YOLO.jpg" class='center'>
<figcaption>Figure 2: input and output of YOLO </figcaption>
</figure>
        </div>
      </div>

<h3>Three kinds of losses</h3>
<ul>
<li><b> Object loss</b> 
  aims to enforce YOLO to predict existence of an OoI inside a given anchor, i.e. $p(O|\mathbf{r}^a_{G_{ij})$ . 
</li>

<li>
  <b> Class loss</b> predicts the category of an object of interest.

  </li>
  <li>
    <b>Coordinate loss</b> encourages YOLO to esitmate precisely 
    the coordinate information of ROIs.
    </li>
</ul>

      

      <h2> How do missing-label instances degrade detection performance?</h2>
      
<p>While UPIs do not contribute to the training of YOLO through their class and coordinate losses, they can adversely contribute through their object loss. 
  In other words, object loss is computed for all anchors of all grids, whether they contain any ground-truths or not. This is not the case for class and coordinate losses.
   <b>Therefore, computation of object loss for the UPIs creates false-negative training signals, leading to a degradation of performance.</b>
    More specifically, during training of YOLO, the detector may be able to correctly localize an UPI, estimating its corresponding anchor with $p(O)\sim1$. But, due to the absence of a ground-truth label associated to an UPI, its true object label is considered zero, i.e. $t=0$, by default.
  Then, such a false-negative training signal forces the network to learn a UPI as a negative or not-interesting object even though the model can correctly localize and recognize it.
   Ultimately, such a false negative signal from a UPI can confuse the network since the LPIs (Labelled Positive Instance) of the same object category forces the network to learn it as an object of interest while the UPIs from the same object category encourage the network to learn it as a not-interesting (negative) object.
 Finally, such false-negative signals can cause a drop in the performance of YOLO.      
 </p>

<h2> How to overcome this performance degradation?</h2>
<p>Generally speaking, there are two solutions for this problem arise from UPIs; 
  ignore the false negative signals (<a href="https://arxiv.org/abs/1806.06986">Wu et. al. </a>) or correct them. In our <a href="https://arxiv.org/abs/2005.11549"> article </a>, we not only correct these false-negative signals, which are arisen from the object loss but also create more training signals from the coordinate and class losses through generating some pseudo-labels for the UPIs. To read a short description of our self-supervised method, click <a href="./Blogs/Our_self_Supervised.html">here </a>.
</p>
     
    </section>

